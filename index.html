<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Samuel Schmidgall</title>

    <meta name="author" content="Samuel Schmidgall">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/robot.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Samuel Schmidgall
                </p>
                <p>
                  Hello there. My name is Samuel Schmidgall. I am a 2nd year PhD student @ <a href="https://www.jhu.edu/">Johns Hopkins University</a> in ECE and a researcher at <a href="https://deepmind.google/">Google Deepmind</a> as part of the <a href="https://research.google/blog/advancing-medical-ai-with-med-gemini/">medical AI team</a>. 
                </p>
                <p>
                  I’m advised by <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en">Rama Chellappa</a> in the <a href="https://aiem.jhu.edu/">JHU Intelligence for Engineering and Medicine Lab (AIEM)</a>. I also work closely with <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a> in the <a href="https://imerse.lcsr.jhu.edu/">JHU Intelligent Medical Robotic Systems and Equipment Lab (IMERSE)</a> and <a href=" https://www.michaelmoor.me/">Michael Moor</a> in the ETH Medical AI Lab. I’m very grateful to have received support from the NSF Graduate Research fellowship (NSF GRFP).
                </p>
                <p>
                  I was previously an intern at <a href="https://med.stanford.edu/">Stanford</a> for medical AI during Summer 2024 and the <a href="https://www.amd.com/en/solutions/ai.html">AMD</a> Gen AI team during Fall 2024.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sschmi46@jhu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/samuel-schmidgall-288632162/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=bQDooZEAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/SRSchmidgall">Twitter | X</a> &nbsp;/&nbsp;
                  <a href="https://github.com/SamuelSchmidgall/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/samjpg.jpeg"><img style="width:80%;max-width:80%;object-fit: cover; border-radius: 40%;" alt="profile photo" src="images/samjpg.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                   Some of my favorite papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



            <tr>
              <tr onmouseout="gp_stop()" onmouseover="gp_start()"  bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gp_image'><video  width=100% height=100% muted autoplay loop>
                    <source src="images/AgentLab.png" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/AgentLab.png' width="170">
                  </div>
                  <script type="text/javascript">
                    function gp_start() {
                      document.getElementById('gp_image').style.opacity = "1";
                    }
  
                    function gp_stop() {
                      document.getElementById('gp_image').style.opacity = "0";
                    }
                    gp_stop()
                  </script>
                </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2501.04227">
                  <span class="papertitle">Agent Laboratory: Using LLM Agents as Research Assistants</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://scholar.google.com/citations?user=xwy6Va4AAAAJ&hl=en">Yusheng Su</a>, <a href="https://scholar.google.com/citations?user=80Jw_w8AAAAJ&hl=en">Ze Wang</a>, <a href="https://scholar.google.com/citations?hl=en&user=iegEnEwAAAAJ">Ximeng Sun</a>, <a href="https://scholar.google.com/citations?hl=en&user=6Abc8OgAAAAJ">Jialian Wu</a>, <a href="https://scholar.google.com/citations?user=nmyIoRMAAAAJ&hl=en">Xiaodong Yu</a>, <a href="https://scholar.google.com/citations?user=IbeXR9cAAAAJ&hl=en">Jiang Liu</a>, <a href="https://scholar.google.com/citations?hl=en&user=bkALdvsAAAAJ">Zicheng Liu</a>, <a href="https://scholar.google.com/citations?hl=en&user=bX1YILcAAAAJ">Emad Barsoum</a>

                <br>
                <em>arXiv preprint arXiv:2501.04227</em>, 2025
                <br>
                
                <p></p>
                <p>Here, we introduce Agent Laboratory, an open-source large language model (LLM) agent framework for accelerating the individual’s ability to perform research. Agent Laboratory takes a human-produced research idea as input and outputs a code repository and a research report. This is accomplished through specialized agents driven by LLMs that collaborate to perform research based on the human’s preference for the agent’s involvement.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/surfels.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2503.04079">
                  <span class="papertitle">Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene Rendering
                  </span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=BA0KydYAAAAJ&hl=en">Idris O. Sunmola</a>, <a href="https://ericzzj1989.github.io/">Zhenjun Zhao</a>, <strong>Samuel Schmidgall</strong>, <a href="https://www.linkedin.com/in/yumeng-wang-186539157/">Yumeng Wang</a>, <a href="https://scheiklp.github.io/">Paul Maria Scheikl</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a> 
                <br>
                <em>arXiv preprint arXiv:2503.04079</em>, 2025
                <br>
                
                <p></p>
                <p>Surgical Gaussian Surfels (SGS) transforms anisotropic point primitives into surface-aligned elliptical splats by constraining the scale component of the Gaussian covariance matrix along the view-aligned axis.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/neurobench.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2304.04640v3">
                  <span class="papertitle">NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems</span>
                </a>
                <br>
                <a href="https://edge.seas.harvard.edu/people/jason-yik">Jason Yik</a>, <a href="https://github.com/korneelf1">Korneel Van den Berghe</a>, ... <strong>Samuel Schmidgall</strong>, ..., <a href="https://edge.seas.harvard.edu/people/vijay-janapa-reddi">Vijay Janapa Reddi</a> 
                <br>
                <em>Nature Communications</em>, 2025
                <br>
                
                <p></p>
                <p>Neurobench is a collaborative effort of nearly 100 co-authors across over 50 institutions in industry and academia, aiming to provide a representative structure for standardizing the evaluation of neuromorphic approaches.</p>
              </td>
            </tr>


            <tr onmouseout="srt_stop()" onmouseover="srt_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='srt_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/knottie.gif" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/knottie.gif' width="180">
                </div>
                <script type="text/javascript">
                  function srt_start() {
                    document.getElementById('srt_image').style.opacity = "1";
                  }

                  function srt_stop() {
                    document.getElementById('srt_image').style.opacity = "0";
                  }
                  srt_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=fNBbEgcfwO">
                  <span class="papertitle">Surgical Robot Transformer (SRT): Imitation Learning for Surgical Subtasks</span>
                </a>
                <br>
                <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://tonyzhaozh.github.io/">Tony Zhao</a>, <strong>Samuel Schmidgall</strong>, <a href="https://malonecenter.jhu.edu/people/anton-deguet/">Anton Deguet</a>, <a href="https://me.jhu.edu/faculty/marin-kobilarov/">Marin Kobilarov</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>8th Annual Conference on Robot Learning (CoRL), 2024 <em style="color:#FF0000";> (Oral presentation, 4.3%)</em></em>
                <br>
                
                <p></p>
                <p>Here, we introduce the Surgical Robot Transformer (SRT). We explore whether surgical manipulation tasks can be learned on the da Vinci robot via imitation learning. We demonstrate our findings through successful execution of three surgical tasks, including tissue manipulation, needle handling, and knot-tying.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/risk_pred.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.medrxiv.org/content/10.1101/2024.11.19.24317577.abstract">
                  <span class="papertitle">Risk Prediction for Non-cardiac Surgery Using the 12-Lead Electrocardiogram: An Explainable Deep Learning Approach</span>
                </a>
                <br>
                <a href="https://carlwharris.github.io/">Carl Harris</a>, <a href="https://hamr.lcsr.jhu.edu/people/anway-pimpalkar/">Anway Pimpalkar</a>, <a href="https://www.linkedin.com/in/ataes/">Ataes Aggarwal</a>, <a href="https://scholar.google.com/citations?user=vkKDNbsAAAAJ&hl=en">Jiyuan Yang</a>, <a href="https://jerry391.github.io/">Xiaojian Chen</a>, <strong>Samuel Schmidgall</strong>, <a href="https://www.linkedin.com/in/sampath-rapuri-9a55b1199/">Sampath Rapuri</a>, <a href="https://www.bme.jhu.edu/people/faculty/joseph-greenstein/">Joseph Greenstein</a>, <a href="https://coverbytaylor.github.io/">Casey Overby Taylor</a>, <a href="https://profiles.hopkinsmedicine.org/provider/robert-david-stevens/2702555">Robert David Stevens</a> 
                <br>
                <em>medRxiv preprint</em>, 2024
                <br>
                
                <p></p>
                <p>To improve on existing noncardiac surgery risk scores, this work proposes a novel approach which leverages features of the preoperative 12-lead electrocardiogram to predict major adverse postoperative outcomes.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/rtras.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2401.00678">
                  <span class="papertitle">General-purpose foundation models for increased autonomy in robot-assisted surgery</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://users.cs.utah.edu/~adk/">Alan Kuntz</a>, <a href="https://profiles.hopkinsmedicine.org/provider/ahmed-e-ghazi/2705724">Ahmed Ezzat Ghazi</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>Nature Machine Intelligence</em>, 2024
                <br>
                
                <p></p>
                <p>This perspective aims to provide a path toward increasing robot autonomy in robot-assisted surgery through the development of a multi-modal, multi-task, vision-language-action model for surgical robots.</p>
              </td>
            </tr>




            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/surgenimg.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2408.14028">
                  <span class="papertitle">SurGen: Text-Guided Diffusion Model for Surgical Video Generation</span>
                </a>
                <br>
                 <a href="https://x.com/joseph_cho1">Joseph Cho</a>, <strong>Samuel Schmidgall</strong>, <a href="https://cyrilzakka.github.io/">Cyril Zakka</a>, <a href="https://mrudangm.github.io/">Mrudang Mathur</a>,  <a href="https://www.rohanshad.com/">Rohan Shad</a>, <a href="https://stanfordhealthcare.org/doctors/h/william-hiesinger.html">William Hiesinger</a>
                <br>
                <em>arXiv preprint arXiv:2408.14028</em>, 2024
                <br>
                
                <p></p>
                <p>This paper introduces SurGen, a text-guided diffusion model tailored for surgical video synthesis, producing the highest resolution and longest duration videos among existing surgical video generation models.</p>
              </td>
            </tr>





            <tr onmouseout="ac_stop()" onmouseover="ac_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ac_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/acbias.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/acbias.png' width="180">
                </div>
                <script type="text/javascript">
                  function ac_start() {
                    document.getElementById('ac_image').style.opacity = "1";
                  }

                  function ac_stop() {
                    document.getElementById('ac_image').style.opacity = "0";
                  }
                  ac_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2405.07960">
                  <span class="papertitle">AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://carlwharris.github.io/">Carl Harris</a>, <a href="https://profiles.stanford.edu/edreis">Eduardo Reis</a>, <a href="https://profiles.hopkinsmedicine.org/provider/jeff-k-jopling/2709700">Jeffrey Jopling</a>, <a href="https://www.michaelmoor.me/">Michael Moor</a>
                <br>
                <em>arXiv preprint arXiv:2405.07960</em>, 2024
                <br>
                
                <p></p>
                <p>AgentClinic turns static medical QA problems into agents in a clinical environment in order to present a more clinically relevant challenge for multimodal language models.</p>
              </td>
            </tr>

            


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/gpvls.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2407.19305">
                  <span class="papertitle">GP-VLS: A general-purpose vision language model for surgery</span>
                </a>
                <br>
                <strong>Samuel Schmidgall*</strong>, <a href="https://x.com/joseph_cho1">Joseph Cho</a>*, <a href="https://cyrilzakka.github.io/">Cyril Zakka</a>, <a href="https://stanfordhealthcare.org/doctors/h/william-hiesinger.html">William Hiesinger</a>
                <br>
                <em>arXiv preprint arXiv:2407.19305</em>, 2024
                <br>
                
                <p></p>
                <p>This paper introduces GP-VLS, a general-purpose vision language model for surgery that integrates medical and surgical knowledge with visual scene understanding.</p>
              </td>
            </tr>



            <tr>
              <tr onmouseout="mit_stop()" onmouseover="mit_start()"  bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mit_image'><video  width=100% height=100% muted autoplay loop>
                    <source src="images/addressing_bias.png" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/addressing_bias.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function mit_start() {
                      document.getElementById('mit_image').style.opacity = "1";
                    }
  
                    function mit_stop() {
                      document.getElementById('mit_image').style.opacity = "0";
                    }
                    mit_stop()
                  </script>
                </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.nature.com/articles/s41746-024-01283-6">
                  <span class="papertitle">Addressing and mitigating cognitive bias in medical language models.</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>,  <a href="https://carlwharris.github.io/">Carl Harris</a>, <a href="https://scholar.google.com/citations?user=UlMP0GEAAAAJ&hl=en">Ime Essien</a>, <a href="https://scholar.google.com/citations?user=YAAp0eMAAAAJ&hl=en">Daniel Olshvang</a>, <a href="https://sites.google.com/view/tawsifurrahman">Tawsifur Rahman</a>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://ncg.ucsc.edu/jason-eshraghian-bio/">Jason Eshraghian</a>, <a href="https://abadirlab.org/dr-peter-abadir/">Peter Abadir</a>, <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en">Rama Chellappa</a>
                <br>
                <em>NPJ Digital Medicine</em>, 2024
                <br>
                
                <p></p>
                <p>The addition of simple cognitive bias prompts significantly degrades performance. We introduce BiasMedQA to evaluate bias robustness on medical QA problems, and demonstrate mitigation techniques.</p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/starrobot.gif' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10610448">
                  <span class="papertitle">Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>,  <a href="https://ncg.ucsc.edu/jason-eshraghian-bio/">Jason Eshraghian</a>,  <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>2024 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024
                <br>
                
                <p></p>
                <p>Surgical Gym is an open-source high performance platform for surgical robot learning where both the physics simulation and reinforcement learning occur directly on the GPU.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/kidneyphant.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2411.02619">
                  <span class="papertitle">Tracking Tumors under Deformation from Partial Point Clouds using Occupancy Networks</span>
                </a>
                <br>
                <a href="https://github.com/somefoo">Pit Henrich</a>, <a href="https://scholar.google.com/citations?user=Wttff78AAAAJ&hl=en">Jiawei Liu</a>, <a href="https://scholar.google.com/citations?hl=en&user=zMFXyfMAAAAJ">Jiawei Ge</a>, <strong>Samuel Schmidgall</strong>, <a href="https://www.researchgate.net/scientific-contributions/Lauren-Shepard-2228142806">Lauren Shepard</a>, <a href="https://scholar.google.com/citations?user=toaErAEAAAAJ&hl=en">Ahmed Ezzat Ghazi</a>, <a href="https://www.sparc.tf.fau.de/person/prof-dr-franziska-mathis-ullrich/">Franziska Mathis-Ullrich</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024
                <br>
                
                <p></p>
                <p>This work introduces an occupancy network-based method for the localization of tumors within kidney phantoms undergoing deformations at interactive speeds.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/ltdltp.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pubs.aip.org/aip/aml/article/2/2/021501/3291446">
                  <span class="papertitle">Brain-inspired learning in artificial neural networks: a review</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://www.jachterberg.com/">Jascha Achterberg</a>, <a href="https://louiskirsch.com/">Louis Kirsch</a>, <a href="https://scholar.google.com/citations?user=cznfeyoAAAAJ&hl=en">Pardis Hajiseyedrazi</a>, <a href="https://ncg.ucsc.edu/jason-eshraghian-bio/">Jason Eshraghian</a>
                <br>
                <em>APL Machine Learning</em>, 2024
                <br>
                
                <p></p>
                <p>Comprehensive review of current brain-inspired learning representations in artificial neural networks.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/surgop.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.nature.com/articles/s41585-024-00873-z">
                  <span class="papertitle">Robots learning to imitate surgeons—challenges and possibilities</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>Nature Reviews Urology</em>, 2024
                <br>
                
                <p></p>
                <p>Autonomous surgical robots have the potential to transform surgery and increase access to quality health care. Advances in artificial intelligence have produced robots mimicking human demonstrations. This application might be feasible for surgical robots but is associated with obstacles in creating robots that emulate surgeon demonstrations.</p>
              </td>
            </tr>





            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/gsvit.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2401.00678">
                  <span class="papertitle">General surgery vision transformer: A video pre-trained foundation model for general surgery</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://profiles.hopkinsmedicine.org/provider/jeff-k-jopling/2709700">Jeffrey Jopling</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>arXiv preprint arXiv:2403.05949</em>, 2024
                <br>
                
                <p></p>
                <p>This paper introduces large video dataset of surgery videos, a general surgery vision transformer (GSViT) pretrained on surgical videos, code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures.</p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/neuroai.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.nature.com/articles/s41467-024-53375-2">
                  <span class="papertitle">Trainees’ perspectives and recommendations for catalyzing the next generation of NeuroAI researchers</span>
                </a>
                <br>
                 <a href="https://x.com/loopyluppi">Andrea I. Luppi</a>, <a href="https://x.com/achterbrain">Jascha Achterberg</a>, <strong>Samuel Schmidgall</strong>, <a href="https://x.com/complexbrains?lang=en">Isil Poyraz Bilgin</a>, <a href="https://x.com/peerherholz">Peer Herholz</a>, <a href="https://x.com/bfockter"> Benjamin Fockter</a>, Andrew Siyoon Ham, <a href="https://sushrutthorat.com/">Sushrut Thorat</a>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://github.com/fmilisav">Filip Milisav</a>, <a href="https://aproca.github.io/">Alexandra M. Proca</a>, <a href="https://x.com/tollehanna?lang=en">Hanna M. Tolle</a>, <a href="https://x.com/laura_e_suarez">Laura E. Suárez</a>, <a href="https://x.com/humanscotti">Paul Scotti</a>, <a href="https://www.linkedin.com/in/helena-gellersen-7560ba119/?originalSubdomain=de">Helena M. Gellersen</a>
                <br>
                <em>Nature Communications</em>, 2024
                <br>
                
                <p></p>
                <p>This paper outline challenges and training needs of junior researchers working across AI and neuroscience. We also provide advice and resources to help trainees plan their NeuroAI careers.</p>
              </td>
            </tr>






            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/robograsp.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=fYRlaylCI3">
                  <span class="papertitle">Learning a Library of Surgical Manipulation Skills for Robotic Surgery</span>
                </a>
                <br>
                <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <strong>Samuel Schmidgall</strong>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>, <a href="https://me.jhu.edu/faculty/marin-kobilarov/">Marin Kobilarov</a>
                <br>
                <em>7th Conference on Robot Learning (CoRL), Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions</em>, 2023
                <br>
                
                <p></p>
                <p> Preliminary progress towards learning a library of surgical manipulation skills using the da Vinci Research Kit (dVRK).</p>
              </td>
            </tr>




            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/selfdiag.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2309.09362">
                  <span class="papertitle">Language models are susceptible to incorrect patient self-diagnosis in medical applications</span>
                </a>
                <br> 
                <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <strong>Samuel Schmidgall</strong>
                <br>
                <em>NeurIPS 2023 Deep Generative Models for Healthcare Workshop</em>, 2023
                <br>
                
                <p></p>
                <p>We show that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.</p>
              </td>
            </tr>




            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/sma.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.01906">
                <span class="papertitle">Synaptic motor adaptation: A three-factor learning rule for adaptive robotic control in spiking neural networks</span>
              </a>
              <br>
              <strong>Samuel Schmidgall</strong>, Joseph Hays
              <br>
              <em>Proceedings of the 2023 International Conference on Neuromorphic Systems</em>, 2023
              <br>
              
              <p></p>
              <p>This paper introduces the Synaptic Motor Adaptation (SMA) algorithm, a novel approach to achieving real-time online adaptation in quadruped robots through the utilization of neuroscience-derived rules of synaptic plasticity with three-factor learning.</p>
            </td>
          </tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/rodent.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1183321/full">
                <span class="papertitle">Meta-SpikePropamine: Learning to learn with synaptic plasticity in spiking neural networks</span>
              </a>
              <br>
              <strong>Samuel Schmidgall</strong>, Joseph Hays
              <br>
              <em>Frontiers in Neuroscience</em>, 2023
              <br>
              
              <p></p>
              <p>We introduce a bi-level optimization framework that seeks to both solve online learning tasks and improve the ability to learn online using models of plasticity from neuroscience.</p>
            </td>
          </tr>

          

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/loco_circuit.png' width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.biorxiv.org/content/10.1101/2022.09.30.510374v2.full.pdf">
                <span class="papertitle">Biological connectomes as a representation for the architecture of artificial neural networks</span>
              </a>
              <br>
              <strong>Samuel Schmidgall</strong>, Catherine Schuman, Maryam Parsa
              <br>
              <em>Proceedings of the 2023 AAAI Conference on Artificial Intelligence "Systems Neuroscience Approach to General Intelligence" Workshop</em>, 2023
              <br>
              
              <p></p>
              <p>We translate the motor circuit of the C. Elegans nematode into artificial neural networks at varying levels of biophysical realism and evaluate the outcome of training these networks on motor and non-motor behavioral tasks.</p>
            </td>
          </tr>

          


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/lockedfronts.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s00285-022-01802-7">
                  <span class="papertitle">Locked fronts in a discrete time discrete space
                    population model.</span>
                </a>
                <br>
                Matthew Holzer, Zachary Richey, Wyatt Rush, <strong>Samuel Schmidgall</strong>
                <br>
                <em>Journal of Mathematical Biology.</em>, 2023
                <br>
                
                <p></p>
                <p>We construct locked fronts for a particular piecewise linear reproduction function. These fronts are shown to be linear combinations of exponentially decaying solutions to the linear system near the unstable state.</p>
              </td>
            </tr>


            <tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/SpikePropGraph.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2021.629210/full">
                  <span class="papertitle">SpikePropamine: Differentiable Plasticity in
                    Spiking Neural Networks.</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>,  Julia Ashkanazy, Wallace Lawson, Joseph Hays
                <br>
                <em>Frontiers in Neurorobotics.</em>, 2021
                <br>
                
                <p></p>
                <p>We introduce a framework for simultaneously learning the underlying fixed-weights and the rules governing the dynamics of synaptic plasticity and neuromodulated synaptic plasticity in SNNs through gradient descent.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/nonholonomic.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9658995">
                  <span class="papertitle">Optimal Localized Trajectory Planning of Multiple Non-holonomic Vehicles</span>
                </a>
                <br>
                Anton Lukyanenko, Heath Camphire, Avery Austin, <strong>Samuel Schmidgall</strong>, Damoon Soudbakhsh
                <br>
                <em>2021 IEEE Conference on Control Technology and Applications (CCTA)</em>, 2021
                <br>
                
                <p></p>
                <p>We present a trajectory planning method for multiple vehicles to navigate a crowded environment, such as a gridlocked intersection or a small parking area.</p>
              </td>
            </tr>




            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/impaired_limb.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2103.15692">
                  <span class="papertitle">Self-Constructing Neural Networks through Random Mutation</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>
                <br>
                <em>ICLR 2021 Never-Ending Reinforcement Learning Workshop</em>, 2021
                <br>
                
                <p></p>
                <p>This paper presents a simple method for learning neural architecture through random mutation.</p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/gecco_il.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2006.05832">
                  <span class="papertitle">Adaptive Reinforcement Learning through Evolving Self-Modifying Neural Networks</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>
                <br>
                <em>Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion.</em>, 2020
                <br>
                
                <p></p>
                <p>We show quadrupedal agents evolved using self-modifying plastic networks are more capable of adapting to complex meta-learning learning tasks, even outperforming the same network updated using gradient-based algorithms while taking less time to train.</p>
              </td>
            </tr>


          
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Original <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>

<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Samuel Schmidgall</title>

    <meta name="author" content="Samuel Schmidgall">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/robot.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Samuel Schmidgall
                </p>
                <p>Hello there, and welcome to my website! My name is Samuel Schmidgall and I’m a researcher & engineer focused on applying AI to the field of medicine and medical robotics.
                </p>
                <p>
                  I am a 2nd year PhD student @ <a href="https://www.jhu.edu/">Johns Hopkins University</a> in Electrical and Computer Engineering. I’m jointly advised by <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en">Rama Chellappa</a> and <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a> in the <a href="https://aiem.jhu.edu/">Intelligence for Engineering and Medicine Lab (AIEM)</a> and the <a href="https://imerse.lcsr.jhu.edu/">Intelligent Medical Robotic Systems and Equipment Lab (IMERSE)</a> toward building autonomous surgical robots and medical language models. I’m very grateful to have received support from the NSF Graduate Research fellowship (NSF GRFP).
                </p>
                <p>
                  I was previously an intern at <a href="https://www.stanford.edu/">Stanford</a> during Summer 2024 and am currently an intern at <a href="https://www.amd.com/en/solutions/ai.html">AMD</a> as part of the generative AI team.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sschmi46@jhu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/samuel-schmidgall-288632162/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=bQDooZEAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/SRSchmidgall">Twitter | X</a> &nbsp;/&nbsp;
                  <a href="https://github.com/SamuelSchmidgall/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/samjpg.jpeg"><img style="width:80%;max-width:80%;object-fit: cover; border-radius: 40%;" alt="profile photo" src="images/samjpg.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                   Some of my favorite papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>







            <tr onmouseout="srt_stop()" onmouseover="srt_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='srt_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/knottie.gif" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/knottie.gif' width="180">
                </div>
                <script type="text/javascript">
                  function srt_start() {
                    document.getElementById('srt_image').style.opacity = "1";
                  }

                  function srt_stop() {
                    document.getElementById('srt_image').style.opacity = "0";
                  }
                  srt_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=fNBbEgcfwO">
                  <span class="papertitle">Surgical Robot Transformer (SRT): Imitation Learning for Surgical Subtasks</span>
                </a>
                <br>
                <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://tonyzhaozh.github.io/">Tony Zhao</a>, <strong>Samuel Schmidgall</strong>, <a href="https://malonecenter.jhu.edu/people/anton-deguet/">Anton Deguet</a>, <a href="https://me.jhu.edu/faculty/marin-kobilarov/">Marin Kobilarov</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>8th Annual Conference on Robot Learning (CoRL)</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>This paper introduces SurGen, a text-guided diffusion model tailored for surgical video synthesis, producing the highest resolution and longest duration videos among existing surgical video generation models.</p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/surgenimg.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2408.14028">
                  <span class="papertitle">SurGen: Text-Guided Diffusion Model for Surgical Video Generation</span>
                </a>
                <br>
                 <a href="https://x.com/joseph_cho1">Joseph Cho</a>, <strong>Samuel Schmidgall</strong>, <a href="https://cyrilzakka.github.io/">Cyril Zakka</a>, <a href="https://mrudangm.github.io/">Mrudang Mathur</a>,  <a href="https://www.rohanshad.com/">Rohan Shad</a>, <a href="https://stanfordhealthcare.org/doctors/h/william-hiesinger.html">William Hiesinger</a>
                <br>
                <em>arXiv preprint arXiv:2408.14028</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>This paper introduces SurGen, a text-guided diffusion model tailored for surgical video synthesis, producing the highest resolution and longest duration videos among existing surgical video generation models.</p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/gpvls.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2407.19305">
                  <span class="papertitle">GP-VLS: A general-purpose vision language model for surgery</span>
                </a>
                <br>
                <strong>Samuel Schmidgall*</strong>, <a href="https://x.com/joseph_cho1">Joseph Cho</a>*, <a href="https://cyrilzakka.github.io/">Cyril Zakka</a>, <a href="https://stanfordhealthcare.org/doctors/h/william-hiesinger.html">William Hiesinger</a>
                <br>
                <em>arXiv preprint arXiv:2407.19305</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>This paper introduces GP-VLS, a general-purpose vision language model for surgery that integrates medical and surgical knowledge with visual scene understanding.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/ltdltp.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pubs.aip.org/aip/aml/article/2/2/021501/3291446">
                  <span class="papertitle">Brain-inspired learning in artificial neural networks: a review</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://www.jachterberg.com/">Jascha Achterberg</a>, <a href="https://louiskirsch.com/">Louis Kirsch</a>, <a href="https://scholar.google.com/citations?user=cznfeyoAAAAJ&hl=en">Pardis Hajiseyedrazi</a>, <a href="https://ncg.ucsc.edu/jason-eshraghian-bio/">Jason Eshraghian</a>
                <br>
                <em>APL Machine Learning</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>Comprehensive review of current brain-inspired learning representations in artificial neural networks.</p>
              </td>
            </tr>






            <tr onmouseout="ac_stop()" onmouseover="ac_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ac_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/acbias.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/acbias.png' width="180">
                </div>
                <script type="text/javascript">
                  function ac_start() {
                    document.getElementById('ac_image').style.opacity = "1";
                  }

                  function ac_stop() {
                    document.getElementById('ac_image').style.opacity = "0";
                  }
                  ac_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2405.07960">
                  <span class="papertitle">AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://carlwharris.github.io/">Carl Harris</a>, <a href="https://profiles.stanford.edu/edreis">Eduardo Reis</a>, <a href="https://profiles.hopkinsmedicine.org/provider/jeff-k-jopling/2709700">Jeffrey Jopling</a>, <a href="https://www.michaelmoor.me/">Michael Moor</a>
                <br>
                <em>arXiv preprint arXiv:2405.07960</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>AgentClinic turns static medical QA problems into agents in a clinical environment in order to present a more clinically relevant challenge for multimodal language models.</p>
              </td>
            </tr>

            


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/surgop.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.nature.com/articles/s41585-024-00873-z">
                  <span class="papertitle">Robots learning to imitate surgeons—challenges and possibilities</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>Nature Reviews Urology</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>Autonomous surgical robots have the potential to transform surgery and increase access to quality health care. Advances in artificial intelligence have produced robots mimicking human demonstrations. This application might be feasible for surgical robots but is associated with obstacles in creating robots that emulate surgeon demonstrations.</p>
              </td>
            </tr>





            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/gsvit.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2401.00678">
                  <span class="papertitle">General surgery vision transformer: A video pre-trained foundation model for general surgery</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://profiles.hopkinsmedicine.org/provider/jeff-k-jopling/2709700">Jeffrey Jopling</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>arXiv preprint arXiv:2403.05949</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>This paper introduces large video dataset of surgery videos, a general surgery vision transformer (GSViT) pretrained on surgical videos, code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures.</p>
              </td>
            </tr>



            <tr>
              <tr onmouseout="mit_stop()" onmouseover="mit_start()"  bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mit_image'><video  width=100% height=100% muted autoplay loop>
                    <source src="images/addressing_bias.png" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/addressing_bias.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function mit_start() {
                      document.getElementById('mit_image').style.opacity = "1";
                    }
  
                    function mit_stop() {
                      document.getElementById('mit_image').style.opacity = "0";
                    }
                    mit_stop()
                  </script>
                </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2402.08113">
                  <span class="papertitle">Addressing and mitigating cognitive bias in medical language models.</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>,  <a href="https://carlwharris.github.io/">Carl Harris</a>, <a href="https://scholar.google.com/citations?user=UlMP0GEAAAAJ&hl=en">Ime Essien</a>, <a href="https://scholar.google.com/citations?user=YAAp0eMAAAAJ&hl=en">Daniel Olshvang</a>, <a href="https://sites.google.com/view/tawsifurrahman">Tawsifur Rahman</a>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <a href="https://ncg.ucsc.edu/jason-eshraghian-bio/">Jason Eshraghian</a>, <a href="https://abadirlab.org/dr-peter-abadir/">Peter Abadir</a>, <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en">Rama Chellappa</a>
                <br>
                <em>NPJ Digital Medicine</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>The addition of simple cognitive bias prompts significantly degrades performance. We introduce BiasMedQA to evaluate bias robustness on medical QA problems, and demonstrate mitigation techniques.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/starrobot.gif' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10610448">
                  <span class="papertitle">Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>,  <a href="https://ncg.ucsc.edu/jason-eshraghian-bio/">Jason Eshraghian</a>,  <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>2024 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>Surgical Gym is an open-source high performance platform for surgical robot learning where both the physics simulation and reinforcement learning occur directly on the GPU.</p>
              </td>
            </tr>

            <tr>
              <tr onmouseout="gp_stop()" onmouseover="gp_start()"  bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gp_image'><video  width=100% height=100% muted autoplay loop>
                    <source src="images/rtras.png" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/rtras.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function gp_start() {
                      document.getElementById('gp_image').style.opacity = "1";
                    }
  
                    function gp_stop() {
                      document.getElementById('gp_image').style.opacity = "0";
                    }
                    gp_stop()
                  </script>
                </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2401.00678">
                  <span class="papertitle">General-purpose foundation models for increased autonomy in robot-assisted surgery</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>, <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <a href="https://users.cs.utah.edu/~adk/">Alan Kuntz</a>, <a href="https://profiles.hopkinsmedicine.org/provider/ahmed-e-ghazi/2705724">Ahmed Ezzat Ghazi</a>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>
                <br>
                <em>Nature Machine Intelligence</em>, 2024
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>This perspective aims to provide a path toward increasing robot autonomy in robot-assisted surgery through the development of a multi-modal, multi-task, vision-language-action model for surgical robots.</p>
              </td>
            </tr>





            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/robograsp.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=fYRlaylCI3">
                  <span class="papertitle">Learning a Library of Surgical Manipulation Skills for Robotic Surgery</span>
                </a>
                <br>
                <a href="https://sites.google.com/view/jkimrobot/home">Ji Woong Kim</a>, <strong>Samuel Schmidgall</strong>, <a href="https://imerse.lcsr.jhu.edu/#professor">Axel Krieger</a>, <a href="https://me.jhu.edu/faculty/marin-kobilarov/">Marin Kobilarov</a>
                <br>
                <em>7th Conference on Robot Learning (CoRL), Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions</em>, 2023
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p> Preliminary progress towards learning a library of surgical manipulation skills using the da Vinci Research Kit (dVRK).</p>
              </td>
            </tr>




            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/selfdiag.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2309.09362">
                  <span class="papertitle">Language models are susceptible to incorrect patient self-diagnosis in medical applications</span>
                </a>
                <br> 
                <a href="https://scholar.google.com/citations?user=hhXu6jkAAAAJ&hl=en">Rojin Ziaei</a>, <strong>Samuel Schmidgall</strong>
                <br>
                <em>NeurIPS 2023 Deep Generative Models for Healthcare Workshop</em>, 2023
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>We show that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.</p>
              </td>
            </tr>




            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/sma.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.01906">
                <span class="papertitle">Synaptic motor adaptation: A three-factor learning rule for adaptive robotic control in spiking neural networks</span>
              </a>
              <br>
              <strong>Samuel Schmidgall</strong>, Joseph Hays
              <br>
              <em>Proceedings of the 2023 International Conference on Neuromorphic Systems</em>, 2023
              <br>
              <a href="data/SOMETHING.bib">bibtex</a>
              <p></p>
              <p>This paper introduces the Synaptic Motor Adaptation (SMA) algorithm, a novel approach to achieving real-time online adaptation in quadruped robots through the utilization of neuroscience-derived rules of synaptic plasticity with three-factor learning.</p>
            </td>
          </tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/rodent.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1183321/full">
                <span class="papertitle">Meta-SpikePropamine: Learning to learn with synaptic plasticity in spiking neural networks</span>
              </a>
              <br>
              <strong>Samuel Schmidgall</strong>, Joseph Hays
              <br>
              <em>Frontiers in Neuroscience</em>, 2023
              <br>
              <a href="data/SOMETHING.bib">bibtex</a>
              <p></p>
              <p>We introduce a bi-level optimization framework that seeks to both solve online learning tasks and improve the ability to learn online using models of plasticity from neuroscience.</p>
            </td>
          </tr>

          

            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/loco_circuit.png' width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.biorxiv.org/content/10.1101/2022.09.30.510374v2.full.pdf">
                <span class="papertitle">Biological connectomes as a representation for the architecture of artificial neural networks</span>
              </a>
              <br>
              <strong>Samuel Schmidgall</strong>, Catherine Schuman, Maryam Parsa
              <br>
              <em>Proceedings of the 2023 AAAI Conference on Artificial Intelligence "Systems Neuroscience Approach to General Intelligence" Workshop</em>, 2023
              <br>
              <a href="data/SOMETHING.bib">bibtex</a>
              <p></p>
              <p>We translate the motor circuit of the C. Elegans nematode into artificial neural networks at varying levels of biophysical realism and evaluate the outcome of training these networks on motor and non-motor behavioral tasks.</p>
            </td>
          </tr>

          


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/lockedfronts.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s00285-022-01802-7">
                  <span class="papertitle">Locked fronts in a discrete time discrete space
                    population model.</span>
                </a>
                <br>
                Matthew Holzer, Zachary Richey, Wyatt Rush, <strong>Samuel Schmidgall</strong>
                <br>
                <em>Journal of Mathematical Biology.</em>, 2023
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>We construct locked fronts for a particular piecewise linear reproduction function. These fronts are shown to be linear combinations of exponentially decaying solutions to the linear system near the unstable state.</p>
              </td>
            </tr>


            <tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/robocheetah.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2402.08113">
                  <span class="papertitle">SpikePropamine: Differentiable Plasticity in
                    Spiking Neural Networks.</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>,  Julia Ashkanazy, Wallace Lawson, Joseph Hays
                <br>
                <em>Frontiers in Neurorobotics.</em>, 2021
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>We introduce a framework for simultaneously learning the underlying fixed-weights and the rules governing the dynamics of synaptic plasticity and neuromodulated synaptic plasticity in SNNs through gradient descent.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/nonholonomic.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9658995">
                  <span class="papertitle">Optimal Localized Trajectory Planning of Multiple Non-holonomic Vehicles</span>
                </a>
                <br>
                Anton Lukyanenko, Heath Camphire, Avery Austin, <strong>Samuel Schmidgall</strong>, Damoon Soudbakhsh
                <br>
                <em>2021 IEEE Conference on Control Technology and Applications (CCTA)</em>, 2021
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>We present a trajectory planning method for multiple vehicles to navigate a crowded environment, such as a gridlocked intersection or a small parking area.</p>
              </td>
            </tr>




            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/impaired_limb.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2103.15692">
                  <span class="papertitle">Self-Constructing Neural Networks through Random Mutation</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>
                <br>
                <em>ICLR 2021 Never-Ending Reinforcement Learning Workshop</em>, 2021
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>This paper presents a simple method for learning neural architecture through random mutation.</p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/gecco_il.png' width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2006.05832">
                  <span class="papertitle">Adaptive Reinforcement Learning through Evolving Self-Modifying Neural Networks</span>
                </a>
                <br>
                <strong>Samuel Schmidgall</strong>
                <br>
                <em>Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion.</em>, 2020
                <br>
                <a href="data/SOMETHING.bib">bibtex</a>
                <p></p>
                <p>We show quadrupedal agents evolved using self-modifying plastic networks are more capable of adapting to complex meta-learning learning tasks, even outperforming the same network updated using gradient-based algorithms while taking less time to train.</p>
              </td>
            </tr>


          
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Original <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
